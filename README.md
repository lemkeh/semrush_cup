# semrush_cup
My solution to semrush cup 1 hackaton

Main problem in this hackathon was that it has too large data, so main emphasis in my solution was to minimize data usage

What've been done:
• Data preprocessing
• Feature engineering
• Model training
<!-- • Clustering via K-Means(I'll add file later) -->

Due to the large data usage my PC's RAM had no chance in all off objectives.
These are some of ideas still haven't been implemented:

• Split url by domain parts than use tf-idf on them
• Split url by domain and count subdomains and path's parts
• Use hash2vec on urls/splitted urls
• Make clustering with other models(DBSCAN and OPTICS)
